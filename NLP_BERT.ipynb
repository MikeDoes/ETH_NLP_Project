{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_Naive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeDoes/ETH_NLP_Project/blob/main/NLP_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBS0rIyembad",
        "outputId": "f0cd77d4-fb15-42b6-a7f6-fab887e716cd"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "\n",
        "import urllib\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForPreTraining, BertConfig  , AdamW  \n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset , DataLoader"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlLX-k-omnDK",
        "outputId": "6bc2f5e7-62be-4a46-bae5-ffb2ca9d527f"
      },
      "source": [
        "\n",
        "!nvidia-smi\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExeavrU09Cvs",
        "outputId": "62006670-f869-4f68-b241-6aaba90f432e"
      },
      "source": [
        "N_EPOCHS=25\n",
        "accumulation_steps=5\n",
        "NUM_EPOCHS=N_EPOCHS\n",
        "BATCH_SIZE=4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
        "MODEL_NAME= BertForPreTraining.from_pretrained('bert-base-cased') #config=config\n",
        "tokenizer= BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojJE0kTsoan9",
        "outputId": "47eced2b-3e1b-423a-edab-9533972d7d41"
      },
      "source": [
        "### DOWNLOAD DATA FROM MIKEDOES (A LOT)\n",
        "\n",
        "\n",
        "url='https://raw.githubusercontent.com/MikeDoes/ETH_NLP_Project/main/fin_num_merged.json'\n",
        "response = urllib.request.urlopen(url)\n",
        "\n",
        "  \n",
        "data=json.loads(response.read())  \n",
        "\n",
        "print(len( data ))\n",
        "\n",
        "with open('/content/data.json', 'w') as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "\n",
        "print(data[0])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "409\n",
            "{'paragraph': \"In care delivery our clinical leaders are applying clinical physician support based on evidence-based guidelines that promote better health and ensure the right care at the right time in the right setting. Today 99% of OptumCare patients in our advanced form of Medicare value arrangements are in 4-star plans or better and OptumCare's average Net Promoter Score is nearly 80 evidence of outstanding clinical outcomes and patient experiences.\", 'entities': [{'target_num': '80', 'category': 'other', 'offset_start': 373, 'offset_end': 375, 'claim': 0}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucFy4Q8VodTN",
        "outputId": "a850f5e9-88c8-4e2b-ae5f-136bba0e346b"
      },
      "source": [
        "###BUILDING PD DATAFRAME\n",
        "\n",
        "def extract_qa( path_1 : Path):\n",
        "  with path_1.open() as json_file:\n",
        "    data=json.load(json_file)\n",
        "  data_rows=[]\n",
        "  for element in data:\n",
        "    paragraph=element[\"paragraph\"]\n",
        "    target=element[\"entities\"][0][\"target_num\"]\n",
        "    category=element[\"entities\"][0][\"category\"]\n",
        "    offset_start=element[\"entities\"][0][\"offset_start\"]\n",
        "    offset_end=element[\"entities\"][0][\"offset_end\"]\n",
        "\n",
        "    data_rows.append({\n",
        "        \"paragraph\":paragraph,\n",
        "        \"target\":target,\n",
        "        \"category\":category,\n",
        "        \"offset_start\":offset_start,\n",
        "        \"offset_end\":offset_end,\n",
        "        \"model_prediction_category\":'',\n",
        "        \"model_prediction_entity\":''\n",
        "\n",
        "    })\n",
        "\n",
        "  return pd.DataFrame(data_rows)\n",
        "      \n",
        "\n",
        "train_df=extract_qa(Path(\"/content/data.json\"))\n",
        "\n",
        "element=train_df.iloc[0]\n",
        "print(element)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paragraph                    In care delivery our clinical leaders are appl...\n",
            "target                                                                      80\n",
            "category                                                                 other\n",
            "offset_start                                                               373\n",
            "offset_end                                                                 375\n",
            "model_prediction_category                                                     \n",
            "model_prediction_entity                                                       \n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqfn4t0fQiHH"
      },
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      data: pd.DataFrame,\n",
        "      tokenizer: tokenizer,\n",
        "      source_max_token_len: int=512,\n",
        "      target_max_token_len: int=8,\n",
        "  ):\n",
        "\n",
        "      self.tokenizer=tokenizer,\n",
        "      self.data=data\n",
        "      self.source_max_token_len=source_max_token_len\n",
        "      self.target_max_token_len=target_max_token_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,index:int):\n",
        "\n",
        "    ##DATA ROW\n",
        "    data_row=self.data.iloc[index]\n",
        "\n",
        "\n",
        "    ##SOURCE ENCODING\n",
        "    source_encoding=tokenizer(\n",
        "    data_row[\"paragraph\"],\n",
        "    max_length=self.source_max_token_len,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True, \n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\" )\n",
        "\n",
        "    ##TARGET ENCODING\n",
        "    target_encoding=tokenizer(\n",
        "    data_row[\"target\"],\n",
        "    max_length=self.target_max_token_len,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True, \n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\" )\n",
        "\n",
        "    labels=target_encoding[\"input_ids\"]\n",
        "    labels[labels==0]=-100\n",
        "\n",
        "\n",
        "    return dict(\n",
        "        input_ids=source_encoding[\"input_ids\"],\n",
        "        attention_mask=source_encoding[\"attention_mask\"],\n",
        "        labels=labels,\n",
        "    )\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZkIAoM1RjPM"
      },
      "source": [
        "class QAmodel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = MODEL_NAME\n",
        "    \n",
        "\n",
        "  def forward(self , input_ids , attention_mask , labels ):\n",
        "\n",
        "    output=self.model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      labels=labels\n",
        "    )   \n",
        "\n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return AdamW(self.parameters(),lr=0.0001)\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vztTrzox7v9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c321c02-036c-4f8b-d7f5-4ac48b135331"
      },
      "source": [
        "\n",
        "dataset=QADataset(train_df,tokenizer,)\n",
        "print(dataset.__len__())\n",
        "data_module = DataLoader(dataset, batch_size=BATCH_SIZE,shuffle=\"True\")\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc3OWYOEpApp"
      },
      "source": [
        "def model_training(data_module):\n",
        "  BERT=QAmodel()\n",
        "\n",
        "\n",
        "  BERT=BERT.to(device)\n",
        "  optimizer=BERT.configure_optimizers()\n",
        "\n",
        "  training_loss=[]\n",
        "  _validation_loss=[]\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  for i in range (NUM_EPOCHS):\n",
        "    print(f\"epoch--{i}\")\n",
        "    loss_count=0\n",
        "    k=0\n",
        "    BERT.train()\n",
        "    for batch in data_module:\n",
        "\n",
        "      input_ids=torch.squeeze(batch[\"input_ids\"])\n",
        "      attention_mask=torch.squeeze(batch[\"attention_mask\"])\n",
        "      labels=torch.squeeze(batch[\"labels\"])\n",
        "\n",
        "      print(input_ids.shape)\n",
        "      print(attention_mask.shape)\n",
        "      print(labels.shape)\n",
        "      \n",
        "      input_ids=input_ids.to(device)\n",
        "      attention_mask=attention_mask.to(device)\n",
        "      labels=labels.to(device)\n",
        "      \n",
        "\n",
        "      output=BERT.forward(input_ids,attention_mask,labels)\n",
        "      loss=output.loss/accumulation_steps\n",
        "      loss.backward()\n",
        "      k+=1\n",
        "\n",
        "      if(k % accumulation_steps == 0):\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        loss_count+=loss.item()*accumulation_steps\n",
        "\n",
        "\n",
        "        if(k % 50 ==0):\n",
        "          mean_loss=loss_count/50\n",
        "          print(f\"average loss --:{mean_loss}\")\n",
        "          training_loss.append(mean_loss)\n",
        "          loss_count=0\n",
        "\n",
        "  return BERT"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHg8ujnCsGLG",
        "outputId": "083db08a-4b21-41b1-c9e8-c273989f98a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "model_training(data_module)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch--0\n",
            "torch.Size([4, 512])\n",
            "torch.Size([4, 512])\n",
            "torch.Size([4, 8])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-e47b9ea729bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-07095cb51024>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(data_module)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBERT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mk\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'int'"
          ]
        }
      ]
    }
  ]
}