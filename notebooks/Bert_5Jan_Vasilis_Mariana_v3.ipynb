{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87318fa5",
      "metadata": {
        "id": "87318fa5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#requirements\n",
        "!pip install sentencepiece\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install nltk\n",
        "\n",
        "import urllib\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForPreTraining, BertConfig, AdamW  \n",
        "\n",
        "from transformers import AutoTokenizer, BertForMaskedLM\n",
        "from transformers import TFBertForTokenClassification, TFTrainer, TFTrainingArguments\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import re #regular expression\n",
        "\n",
        "import string \n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "# /https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForTokenClassification.**kwargs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "18411ed7",
      "metadata": {
        "id": "18411ed7"
      },
      "outputs": [],
      "source": [
        "#PARAMETERS\n",
        "# special tokens\n",
        "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"]\n",
        "# special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
        "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
        "vocab_size = 30_522\n",
        "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
        "max_length = 512\n",
        "# whether to truncate\n",
        "truncate_longer_samples = True\n",
        "EPOCHS=25\n",
        "ACCUM_STEPS=5\n",
        "NUM_EPOCHS=100\n",
        "BATCH_SIZE=4\n",
        "SEED =42\n",
        "RETURN_TENSORS = False\n",
        "LEARNING_RATE = 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "90534bbf",
      "metadata": {
        "id": "90534bbf"
      },
      "outputs": [],
      "source": [
        "# Define tokenizer\n",
        "TOKENIZER = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "# TOKENIZER = WhitespaceTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "23fd5538",
      "metadata": {
        "id": "23fd5538"
      },
      "outputs": [],
      "source": [
        "# Download data\n",
        "\n",
        "##fn has unique paragraphs per row 409\n",
        "url='https://raw.githubusercontent.com/MikeDoes/ETH_NLP_Project/main/fin_num_merged.json'\n",
        "response = urllib.request.urlopen(url)\n",
        "data_fn=json.loads(response.read())\n",
        "\n",
        "##fn3 has duplicated paragraphs per number found 1100 original\n",
        "url='https://raw.githubusercontent.com/MikeDoes/ETH_NLP_Project/main/FinNum-3_ConCall_dev.json'\n",
        "response = urllib.request.urlopen(url)\n",
        "data_fn3=json.loads(response.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "061a89e4",
      "metadata": {
        "id": "061a89e4"
      },
      "outputs": [],
      "source": [
        "#PRE PROCESS Part I\n",
        "def convertdata_pd(data, tokenize_labels=False, tokenizer=TOKENIZER,return_tensors=RETURN_TENSORS):\n",
        "    \"\"\"\n",
        "    Converts the json to a pd. \n",
        "    arg: data set finNum (dics and shape 409)\n",
        "    outputs: 4 >> pd dataframe with X and Y, another with X, another with Y and the list of categories found in the data set.\n",
        "\n",
        "    optionally it can perform the tokenization on the labels already and/or can return tensors format\n",
        "    \"\"\"\n",
        "    data_rows=[]\n",
        "    labels =[]\n",
        "    #data_fn\n",
        "    if data == data_fn:\n",
        "        for i in range(len(data)):\n",
        "            temp = []\n",
        "            paragraph = data[i][\"paragraph\"]\n",
        "            numbers = data[i][\"entities\"]\n",
        "            for j in range(len(numbers)):\n",
        "                num = numbers[j][\"target_num\"]\n",
        "                category = numbers[j][\"category\"]\n",
        "                offset_start = numbers[j][\"offset_start\"]\n",
        "                if tokenize_labels==False:\n",
        "                    temp.append({\n",
        "                        \"number\":num,\n",
        "                        \"label\":category,\n",
        "                        \"pos\":offset_start})\n",
        "                    labels.append({\"labels\":category})\n",
        "                elif return_tensors == False:\n",
        "                    category_token = tokenizer(list(category.split(\" \"))) #tokenize the category \n",
        "                    temp.append({\n",
        "                        \"number\":num,\n",
        "                        \"label\":category_token[\"input_ids\"][0][1],\n",
        "                        \"pos\":offset_start})\n",
        "                    labels.append({\"labels\":category_token[\"input_ids\"][0][1]})\n",
        "                else:\n",
        "                    category_token = tokenizer(list(category.split(\" \")),return_tensors='tf') #tokenize the category\n",
        "                    temp.append({\n",
        "                        \"number\":num,\n",
        "                        \"label\":category_token[\"input_ids\"][0][1],\n",
        "                        \"pos\":offset_start})\n",
        "                    labels.append({\"labels\":category_token[\"input_ids\"][0][1]})\n",
        "                     # add the category. Warning: category can only be one word\n",
        "            data_rows.append({\n",
        "                \"paragraph\":paragraph,\n",
        "                \"labels\":temp\n",
        "            })\n",
        "    #data_fn3\n",
        "    elif data == data_fn3:\n",
        "        for i in range(len(data)):\n",
        "            paragraph=data[i][\"paragraph\"]\n",
        "            target=data[i][\"target_num\"]\n",
        "            category=data[i][\"category\"]\n",
        "            offset_start=data[i][\"offset_start\"]\n",
        "            offset_end=data[i][\"offset_end\"]\n",
        "            data_rows.append({\n",
        "                \"paragraph\":paragraph,\n",
        "                \"target\":target,\n",
        "                \"category\":category,\n",
        "                \"offset_start\":offset_start,\n",
        "                \"offset_end\":offset_end,\n",
        "                \"model_prediction_category\":'',\n",
        "                \"model_prediction_entity\":''\n",
        "            })\n",
        "    x = pd.DataFrame(data_rows).iloc[:,0]\n",
        "    y = pd.DataFrame(data_rows).iloc[:,1]\n",
        "    \n",
        "    labels.append({\"labels\":0}) #decided for 0 as everything is already a token, therefore assumed NULL==0\n",
        "    labels_uniq =[]\n",
        "    if tokenize_labels == False:\n",
        "        labels_uniq=pd.DataFrame(labels).labels.unique()\n",
        "        #todo: check before 0 is a token for padding, so probably we can t add as 0 \n",
        "    \n",
        "    return pd.DataFrame(data_rows), x, y, labels_uniq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e0ef15b2",
      "metadata": {
        "id": "e0ef15b2"
      },
      "outputs": [],
      "source": [
        "##Example\n",
        "data_rows, x ,y, cat =convertdata_pd(data_fn,tokenize_labels=False,return_tensors='tf')\n",
        "# data_rows, x ,y,cat =convertdata_pd(data_fn,tokenize_labels=True,return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cb67693b",
      "metadata": {
        "id": "cb67693b"
      },
      "outputs": [],
      "source": [
        "# PREPROCESS Part II Prep for token\n",
        "def clean(par):\n",
        "    \"\"\"\n",
        "    args: str\n",
        "    receives a paragraph and cleans its text\n",
        "    example: data_rows.iloc[0,0]    \n",
        "    \"\"\"\n",
        "    par\n",
        "    par = par.lower() #remove capital letters\n",
        "    par = par.replace(\"[^a-zA-Z]\", \" \") #remove non english characters\n",
        "    # remove pontuation, as the punctuation i also converted into tokens\n",
        "    for char in par:\n",
        "        if char in string.punctuation:\n",
        "            par = par.replace(char,\"\")\n",
        "    return par\n",
        "            \n",
        "#todo implement for non english https://pypi.org/project/langdetect/\n",
        "#todo keep % as it is important for identification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215c4c76",
      "metadata": {
        "id": "215c4c76"
      },
      "outputs": [],
      "source": [
        "##Example\n",
        "parpar = clean (data_rows.iloc[0,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "cf1b4c35",
      "metadata": {
        "id": "cf1b4c35"
      },
      "outputs": [],
      "source": [
        "# PREPROCESS Part III \n",
        "## Construct target vector\n",
        "def constructor2(par,target,categories,tokenizer=TOKENIZER, return_tensors=RETURN_TENSORS):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      par: paragraph (original paragraph so we can calculate the position \n",
        "      targets: format [{'number': '80', 'label': 'percentage', 'pos': 373}]\n",
        "      categories: full list of categories\n",
        "    Outputs: a vector of the same size as the tokenized paragraph, with 0s and the label in the word positioning\n",
        "    \"\"\"\n",
        "    \n",
        "    #clean the paragraph\n",
        "    par_clean=clean(par)\n",
        "    par_token=tokenizer(par_clean)[\"input_ids\"]\n",
        "    \n",
        "    #initiates a vector with the same size as the tokenized paragraph\n",
        "    y_vector = np.zeros(len(par_token)) \n",
        "    y_vector_numb = np.zeros(len(par_token))\n",
        "\n",
        "    #if the y (target) has still the original categories and not tokenized\n",
        "    for i in range(len(target)):\n",
        "      pos=target[i][\"pos\"] #get the position of the number\n",
        "      spl = par[0:pos] #split the original paragraph untill position of wanted number\n",
        "      spl_clean = clean(spl) #now we clean the splitted paragraph\n",
        "              \n",
        "      # tokenize the splitted and clean paragraph and calculate the length of the tokenized vector\n",
        "      if return_tensors == False:\n",
        "        spl_token = tokenizer(spl_clean)[\"input_ids\"]\n",
        "        size = len(spl_token)-1 #-1 to remove token=102 which is the end of the vector\n",
        "      else:\n",
        "        spl_token = tokenizer(spl_clean, return_tensors='tf')[\"input_ids\"]\n",
        "        size = len(spl_token[0])-1  #-1 to remove token=102 which is the end of the vector\n",
        "              \n",
        "      #in the equivalent position of the number found, we add the category label, which is given by the position in the list\n",
        "      y_vector[size] = np.where(categories==target[i][\"label\"])[0][0] \n",
        "      y_vector_numb[size] = target[i][\"number\"]\n",
        "   \n",
        "    y_vector.tolist()\n",
        "    if return_tensors == 'tf':\n",
        "        par_token=tokenizer(par_clean,return_tensors='tf') #todo: check if we need to add max_length =512, truncation=True, padding ='max_length')above and how to handle\n",
        "        y_vector=tf.convert_to_tensor(y_vector,np.float32)\n",
        "     \n",
        "    return par_token, y_vector, y_vector_numb #todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "bfcc2bc9",
      "metadata": {
        "id": "bfcc2bc9"
      },
      "outputs": [],
      "source": [
        "#Example\n",
        "x_3=x[3]\n",
        "y_3=y[3]\n",
        "par,y_v,y_token=constructor2(x_3,y_3,cat, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL\n",
        "\n",
        "#Parameters\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "bert_model = \"bert-base-cased\"\n",
        "num_labels = 10\n",
        "\n",
        "##Model\n",
        "model = TFBertForTokenClassification.from_pretrained( bert_model , from_pt = True) #https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForTokenClassification.**kwargs\n",
        "model.config.num_labels = 10\n",
        "\n",
        "model.layers[-1].activation = tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax, trainable=True) \n",
        "model.layers[0].trainable = False \n",
        "\n",
        "model.compile(optimizer=optimizer, loss = loss, metrics = [metrics])"
      ],
      "metadata": {
        "id": "tfSEiQN_6U4P"
      },
      "id": "tfSEiQN_6U4P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN MODEL: Approach 1 basic\n",
        "\n",
        "#define train data\n",
        "data_train = data_rows.iloc[0:404]\n",
        "data_eval = data_rows.iloc[404:]\n",
        "EPOCHS=1\n",
        "\n",
        "# train point by point\n",
        "for j in range(EPOCHS):\n",
        "  for i in range(len(data_train)):\n",
        "    par,y_v,y_token=constructor2(data_train[\"paragraph\"][i],data_train[\"labels\"][i],cat, return_tensors='tf')\n",
        "    oneinput = par\n",
        "    oneinput[\"labels\"] = tf.reshape(y_v, (-1, tf.size(y_v)))\n",
        "    model.fit(oneinput[\"input_ids\"][0], oneinput[\"labels\"][0])\n",
        "\n",
        "# the output of the model are probabilities of categories check here https://www.tensorflow.org/tutorials/keras/classification"
      ],
      "metadata": {
        "id": "K4YiMcMT6ppY"
      },
      "id": "K4YiMcMT6ppY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATE THE MODEL\n",
        "test_loss, test_acc = model.evaluate(oneinput[\"input_ids\"][0], oneinput[\"labels\"][0])"
      ],
      "metadata": {
        "id": "XoBLNXU58Pc3"
      },
      "id": "XoBLNXU58Pc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OVERVIEW OF ONE DATA POINT\n",
        "paragraph,y_vector,y_token=constructor2(data_rows[\"paragraph\"][3],data_rows[\"labels\"][3],cat, return_tensors='tf')\n",
        "inn = paragraph\n",
        "inn[\"labels\"] = tf.reshape(y_vector, (-1, tf.size(y_vector)))\n",
        "print(\"PARAGRAPH\",len(data_rows[\"paragraph\"][3]), data_rows[\"paragraph\"][3])\n",
        "print(\"TOKENIZED\", len(par),par)\n",
        "print(\"VECTOR CATEGORIES\",len(y_v),y_v)\n",
        "print(\"OUTPUT\",model.predict(inn[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "W58oF3Np8PI7"
      },
      "id": "W58oF3Np8PI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THIS IS SUPPORT INFORMATION**"
      ],
      "metadata": {
        "id": "MoS1-iBD8QwP"
      },
      "id": "MoS1-iBD8QwP"
    },
    {
      "cell_type": "code",
      "source": [
        "# SUPPORT: other things tried out in the model\n",
        "\n",
        "# model.summary()\n",
        "# configg = BertConfig.from_pretrained(bert_model,num_labels =10)\n",
        "# model.hidden_size = 1\n",
        "\n",
        "# model.layers[-1].activation = tf.keras.activations.softmax ##ERROR\n",
        "# model.layers[-1].activation = tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax, trainable=True) #\"relu\" tf.keras.activations.softmax ### RETURNS 1 85 10  BEST\n",
        "\n",
        "# model.layers[-1].activation = tf.keras.layers.Dense(32, activation=tf.keras.activations.softmax, trainable=True) #\"relu\" tf.keras.activations.softmax ### RETURNS 1 85 32\n",
        "# model.layers[-1].activation = tf.keras.layers.Dense(32, activation='relu', trainable=True) #\"relu\" tf.keras.activations.softmax ### RETURNS 1 85 32\n",
        "# a = tf.keras.layers.Dense(10, activation='silu', trainable=True) #\"relu\" tf.keras.activations.softmax ### RETURNS 1 85 32\n",
        "\n",
        "\n",
        "# model.layers[-1].activation = tf.keras.layers.Maximum() #\"relu\" tf.keras.activations.softmax ## ERROR\n",
        "# inputt = tf.keras.layers.Input(shape=(85,10))\n",
        "# x2 = tf.keras.layers.Dense(8, activation='relu')(inputt)\n",
        "# x1 = tf.keras.layers.Dense(8, activation='relu')(inputt)\n",
        "# model.layers[-1] = tf.keras.layers.Add()(x2)\n",
        "# model.layers[-1].activation = tf.keras.layers.Maximum()(x1)\n",
        "\n",
        "# model.layers[-1].activation = nn.Linear(85, 10) ##error\n",
        "# model.classifier = tf.keras.layers.Linear(85, trainable=True)\n",
        "\n",
        "# model.layers[-1].trainable = True \n",
        "# model.layers[0].trainable = False \n",
        "\n",
        "# model.compile(optimizer=optimizer, loss = loss, metrics = [metrics])\n",
        "# model.summary()\n",
        "# model.config\n",
        "# model.save('bert-base-cased')\n",
        "\n",
        "\n",
        "### options on how to train the model\n",
        "# 1 https://stackoverflow.com/questions/62797376/tensorflow-bert-for-token-classification-exclude-pad-tokens-from-accuracy-whil\n",
        "# model.fit(inputs2)\n",
        "# 2 https://huggingface.co/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel.train_step\n",
        "# model.train_step(inputs2)\n",
        "# 3\n",
        "# outputs = model(inputs2,output_hidden_states=True, return_dict =True, training = True)\n",
        "\n",
        "#https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209\n",
        "\n",
        "# # Clemente https://huggingface.co/docs/transformers/model_doc/bert\n",
        "# model = TFBertForTokenClassification.from_pretrained('bert-base-cased',config= configg)\n",
        "# model.compile(optimizer=optimizer, loss = loss, metrics = [metrics])\n",
        "# # model = TFBertForTokenClassification.from_pretrained('bert-base-cased')\n",
        "# outs = model(inputs2)\n",
        "# outs.loss\n",
        "# a = np.argmax(cc)"
      ],
      "metadata": {
        "id": "NU-sAvQ46n6b"
      },
      "id": "NU-sAvQ46n6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for model\n",
        "inputs_t=[]\n",
        "labels_t=[]\n",
        "for i in range(2):\n",
        "  par,y_v,y_token=constructor2(data_rows[\"paragraph\"][i],data_rows[\"labels\"][i],cat, return_tensors='tf')\n",
        "  # oneinput['input_ids'] = par['input_ids'][0]\n",
        "  oneinput = par\n",
        "  oneinput[\"labels\"] = tf.reshape(y_v, (-1, tf.size(y_v)))\n",
        "  # model.fit(oneinput[\"input_ids\"][0], oneinput[\"labels\"][0])\n",
        "  # print(i)\n",
        "  inputs_t.append(oneinput[\"input_ids\"][0])\n",
        "  labels_t.append(oneinput[\"labels\"][0])"
      ],
      "metadata": {
        "id": "C6inETH5p70T"
      },
      "id": "C6inETH5p70T",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(list, epochs=5)\n",
        "inputs2 = par\n",
        "#inputs2[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(y_v).numpy()), (-1, tf.size(y_v)))\n",
        "inputs2[\"labels\"] = tf.reshape(y_v, (-1, tf.size(y_v)))\n",
        "model.fit(inputs2[\"input_ids\"][0], inputs2[\"labels\"][0], epochs=3)\n",
        "outputs=model.predict(inputs2[\"input_ids\"])\n",
        "outputs\n",
        "out['logits'][0]"
      ],
      "metadata": {
        "id": "PSgh5LRVo__6"
      },
      "id": "PSgh5LRVo__6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {'accuracy': acc,'f1': f1,'precision': precision,'recall': recall}"
      ],
      "metadata": {
        "id": "gr12sw-P6iM7"
      },
      "id": "gr12sw-P6iM7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/huggingface/transformers/issues/8292\n",
        "## this seems now to be outdated\n",
        "training_args = TFTrainingArguments(\n",
        "output_dir='./bert_test', # output directory\n",
        "num_train_epochs=5, # total # of training epochs\n",
        "per_device_train_batch_size=32, # batch size per device during training\n",
        "per_device_eval_batch_size=32, # batch size for evaluation\n",
        "warmup_steps=500, # number of warmup steps for learning rate scheduler\n",
        "weight_decay=0.01, # strength of weight decay\n",
        "logging_dir='./logs', # directory for storing logs\n",
        "learning_rate=3e-5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "s16KAZ7w6wpS"
      },
      "id": "s16KAZ7w6wpS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de28903",
      "metadata": {
        "id": "7de28903"
      },
      "outputs": [],
      "source": [
        "# tips for model\n",
        "# use AdamW optimizer\n",
        "# you can get the loss via\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits\n",
        "\n",
        "#\n",
        "for epoch in range(2):\n",
        "    loop = tqdm(loader, leave = True)\n",
        "    for batch in loop:\n",
        "        optim.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = output.loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bert_5Jan_Vasilis Mariana v3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}