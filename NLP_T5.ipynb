{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_Naive.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeDoes/ETH_NLP_Project/blob/main/NLP_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBS0rIyembad",
        "outputId": "d6c928a4-0b1f-4efc-d05d-78d5b3c2cf60"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "\n",
        "import json\n",
        "import urllib\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import T5ForConditionalGeneration , T5Tokenizer  , AdamW\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset , DataLoader"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlLX-k-omnDK",
        "outputId": "b8281338-4273-4fed-c670-d9b015b5e9e5"
      },
      "source": [
        "\n",
        "!nvidia-smi\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 13 08:26:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    57W / 149W |   5209MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExeavrU09Cvs"
      },
      "source": [
        "N_EPOCHS=25\n",
        "accumulation_steps=1\n",
        "NUM_EPOCHS=N_EPOCHS\n",
        "BATCH_SIZE=8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME=\"t5-small\"\n",
        "tokenizer=T5Tokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojJE0kTsoan9"
      },
      "source": [
        "#DOWLOAND AND PROCESS TRAINING DATASET\n",
        "\n",
        "url='https://raw.githubusercontent.com/MikeDoes/ETH_NLP_Project/main/fin_num_train.json'\n",
        "response = urllib.request.urlopen(url)\n",
        "unprocessed_traindataset=json.loads(response.read())  \n",
        "\n",
        "processed_traindataset=[]\n",
        "\n",
        "for element in unprocessed_traindataset:\n",
        "  input=element['paragraph']\n",
        "  target=''\n",
        "  for entity in element['entities']:\n",
        "    target=target + entity['target_num'] + '-' + entity['category'] + ' $ '\n",
        "  # print(entity)\n",
        "  # print(target)\n",
        "  processed_traindataset.append({\n",
        "      'input':input,\n",
        "      'target': target\n",
        "  })\n",
        "\n",
        "with open('/content/train_dataset.json', 'w') as f:\n",
        "    json.dump(processed_traindataset, f)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DOWLOAND AND PROCESS TRAINING DATASET\n",
        "\n",
        "url='https://raw.githubusercontent.com/MikeDoes/ETH_NLP_Project/main/fin_num_validate.json'\n",
        "response = urllib.request.urlopen(url)\n",
        "unprocessed_traindataset=json.loads(response.read())  \n",
        "\n",
        "processed_traindataset=[]\n",
        "\n",
        "for element in unprocessed_traindataset:\n",
        "  input=element['paragraph']\n",
        "  target=''\n",
        "  for entity in element['entities']:\n",
        "    target=target + entity['target_num'] + '-' + entity['category'] + ' $ '\n",
        "  # print(entity)\n",
        "  # print(target)\n",
        "  processed_traindataset.append({\n",
        "      'input':input,\n",
        "      'target': target\n",
        "  })\n",
        "\n",
        "with open('/content/validate_dataset.json', 'w') as f:\n",
        "    json.dump(processed_traindataset, f)"
      ],
      "metadata": {
        "id": "68a4SFD9mBUE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucFy4Q8VodTN"
      },
      "source": [
        "###BUILDING PD DATAFRAME\n",
        "\n",
        "def extract_dataframe( path_1 : Path):\n",
        "  with path_1.open() as json_file:\n",
        "    data=json.load(json_file)\n",
        "  data_rows=[]\n",
        "  for element in data:\n",
        "    input=element['input']\n",
        "    target=element['target']\n",
        "\n",
        "    data_rows.append({\n",
        "        'input' : input ,\n",
        "        'target' : target ,\n",
        "    })\n",
        "\n",
        "  return pd.DataFrame(data_rows)\n",
        "      \n",
        "\n",
        "train_df=extract_dataframe(Path(\"/content/train_dataset.json\"))\n",
        "validate_df=extract_dataframe(Path(\"/content/validate_dataset.json\"))\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqfn4t0fQiHH"
      },
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(\n",
        "      self,\n",
        "      data: pd.DataFrame,\n",
        "      tokenizer: T5Tokenizer,\n",
        "      source_max_token_len: int=396,\n",
        "      target_max_token_len: int=32,\n",
        "  ):\n",
        "\n",
        "      self.tokenizer=tokenizer,\n",
        "      self.data=data\n",
        "      self.source_max_token_len=source_max_token_len\n",
        "      self.target_max_token_len=target_max_token_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,index:int):\n",
        "\n",
        "    ##DATA ROW\n",
        "    data_row=self.data.iloc[index]\n",
        "\n",
        "\n",
        "    ##SOURCE ENCODING\n",
        "    source_encoding=tokenizer(\n",
        "    data_row['input'],\n",
        "    max_length=self.source_max_token_len,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True, \n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\" )\n",
        "\n",
        "    \n",
        "\n",
        "    ##TARGET ENCODING\n",
        "    target_encoding=tokenizer(\n",
        "    data_row[\"target\"] ,\n",
        "    max_length=self.target_max_token_len,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True, \n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\" )\n",
        "\n",
        "    labels=target_encoding[\"input_ids\"]\n",
        "    labels[labels==0]=-100\n",
        "\n",
        "\n",
        "    return dict(\n",
        "        input_ids=source_encoding[\"input_ids\"],\n",
        "        attention_mask=source_encoding[\"attention_mask\"],\n",
        "        labels=labels,\n",
        "    )\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZkIAoM1RjPM"
      },
      "source": [
        "class QAmodel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME , return_dict=True).to(device)\n",
        "    \n",
        "\n",
        "  def forward(self , input_ids , attention_mask , labels ):\n",
        "\n",
        "    output=self.model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      labels=labels\n",
        "    )   \n",
        "\n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return AdamW(self.parameters(),lr=0.0001)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vztTrzox7v9z"
      },
      "source": [
        "\n",
        "train_dataset=QADataset(train_df,tokenizer,)\n",
        "validate_dataset=QADataset(validate_df,tokenizer)\n",
        "\n",
        "train_module = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=\"True\", drop_last=True)\n",
        "validate_module = DataLoader(validate_dataset, batch_size=BATCH_SIZE , shuffle=\"True\" , drop_last= True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc3OWYOEpApp"
      },
      "source": [
        "def model_training(data_module):\n",
        "  T5=QAmodel()\n",
        "\n",
        "\n",
        "  T5=T5.to(device)\n",
        "  optimizer=T5.configure_optimizers()\n",
        "\n",
        "  training_loss=[]\n",
        "  _validation_loss=[]\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  for i in range (NUM_EPOCHS):\n",
        "    print(f\"epoch--{i}\")\n",
        "    loss_count=0\n",
        "    validation_loss=0\n",
        "    k=0\n",
        "    T5.train()\n",
        "    for batch in data_module:\n",
        "\n",
        "      input_ids=torch.squeeze(batch[\"input_ids\"])\n",
        "      attention_mask=torch.squeeze(batch[\"attention_mask\"])\n",
        "      labels=torch.squeeze(batch[\"labels\"])\n",
        "      \n",
        "      input_ids=input_ids.to(device)\n",
        "      attention_mask=attention_mask.to(device)\n",
        "      labels=labels.to(device)\n",
        "      \n",
        "\n",
        "      output=T5.forward(input_ids,attention_mask,labels)\n",
        "      loss=output.loss/accumulation_steps\n",
        "      loss.backward()\n",
        "      k+=1\n",
        "\n",
        "      if(k % accumulation_steps == 0):\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        loss_count+=loss.item()*accumulation_steps\n",
        "        \n",
        "        if(k % 10 ==0):\n",
        "          mean_loss=loss_count/10\n",
        "          print(f\"average loss --:{mean_loss}\")\n",
        "          training_loss.append(mean_loss)\n",
        "          loss_count=0\n",
        "\n",
        "      \n",
        "    k=0\n",
        "    T5.eval()\n",
        "    for batch in validate_module:\n",
        "\n",
        "      input_ids=torch.squeeze(batch[\"input_ids\"])\n",
        "      attention_mask=torch.squeeze(batch[\"attention_mask\"])\n",
        "      labels=torch.squeeze(batch[\"labels\"])\n",
        "        \n",
        "      input_ids=input_ids.to(device)\n",
        "      attention_mask=attention_mask.to(device)\n",
        "      labels=labels.to(device)\n",
        "        \n",
        "      k=k+1\n",
        "      output=T5.forward(input_ids,attention_mask,labels)\n",
        "      validation_loss+=loss.item()  \n",
        "\n",
        "    print(f\"validation loss: {validation_loss/k} \") \n",
        "\n",
        "  return T5"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHg8ujnCsGLG"
      },
      "source": [
        "model=model_training(train_module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prediction(data_row,model):\n",
        "  source_encoding=tokenizer(\n",
        "    data_row[\"input\"],\n",
        "    max_length=512,\n",
        "    padding=\"max_length\",\n",
        "    truncation=\"only_second\", \n",
        "    return_attention_mask=True,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors=\"pt\" )\n",
        "  source_encoding=source_encoding.to(device)\n",
        "\n",
        "  generated_ids = model.model.generate(\n",
        "      input_ids=source_encoding[\"input_ids\"],\n",
        "      attention_mask=source_encoding[\"attention_mask\"],\n",
        "      num_beams=1,\n",
        "      max_length=128,\n",
        "      min_length=5,\n",
        "      top_p=0.9,\n",
        "      repetition_penalty=2.5,\n",
        "      length_penalty=1.5,\n",
        "      early_stopping=True,\n",
        "      use_cache=True\n",
        "\n",
        "  )\n",
        "\n",
        "  preds=[\n",
        "         tokenizer.decode(generated_id , skip_special_tokens=True , clean_up_tokenization_spaces=True)\n",
        "         for generated_id in generated_ids\n",
        "  ]\n",
        "\n",
        "  return \"\".join(preds)"
      ],
      "metadata": {
        "id": "mraOf8T-4dMk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample=train_df.iloc[1]\n",
        "print(generate_prediction(sample,model))\n",
        "print( sample[\"target\"])\n",
        "print( sample[\"input\"])"
      ],
      "metadata": {
        "id": "MbOfKqf15R4v",
        "outputId": "d424ebc7-cf45-43af-c14a-cf2b0f4779cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018-date $ 10-change $ 9-change $ 1.5-change $ 8.2-money $ 2019.-date $ 23-relative $ 2018.-\n",
            "2018-date $ 10-change $ 9-change $ 1.5-change $ 8.2-money $ 2019.-date $ 23-relative $ 2017.-date $ 11-relative $ 100-money $ \n",
            "Turning to Optum's financial results. Full year 2018 revenues surpassed $100 billion for the first time. Revenue growth of over $10 billion for the year accelerated to 11% from 9% in 2017. And likewise our operating margins once again strengthened across the Optum portfolio with our overall operating earnings growing more than $1.5 billion or 23% to $8.2 billion reflecting the leverage of Optum's scale businesses and putting us in a strong baseline earnings position entering 2019.\n"
          ]
        }
      ]
    }
  ]
}